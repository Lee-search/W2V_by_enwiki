{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efb342f-080c-4c04-9c33-024992c0aac4",
   "metadata": {},
   "source": [
    "# 영어 위키피디아 텍스트 전처리\n",
    "### 텍스트 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e065ba2-32a8-4d07-99d2-8a03082eb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import multiprocessing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe54c20f-733d-4eb8-8586-bee9cd0549fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_tokenizer(sentence):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(sentence)\n",
    "    \n",
    "    result = []\n",
    "     \n",
    "    for word in word_tokens:\n",
    "        if word not in stop_words: \n",
    "            result.append(word)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcf980a-71ca-4054-b331-17e3f6fb7107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파싱한 텍스트를 전처리 한 후 step2 폴더에 저장하는 함수\n",
    "def corpus_preprocessor(data):\n",
    "    \n",
    "    xml_tag = re.compile(r'<[^>]*>')\n",
    "    allowed_char = re.compile(r'[^A-z^0-9^.^,^?^!^ ]')\n",
    "    \n",
    "    path, file_name = data\n",
    "    print(\"process file: {}\\n\".format(file_name))\n",
    "    with open(os.path.join(path, file_name), 'r', encoding='utf-8') as input:\n",
    "        with open(os.path.join(os.getcwd(), 'enwiki_step2', file_name), 'w', encoding='utf-8') as output: \n",
    "            # Line 단위로 Sentence 취급\n",
    "            for line in input: \n",
    "                \n",
    "                # [^문자] ^ 뒤의 문자를 제외한 모든 문자 매치\n",
    "                # re.sub() - 정규 표현식과 일치하는 문자열을 다른 문자열로 대체\n",
    "                sentence = xml_tag.sub(' ', line)\n",
    "                allowed_sentence = allowed_char.sub(' ', sentence)\n",
    "                tokenized_sentence = sentence_tokenizer(allowed_sentence.lower())\n",
    "                \n",
    "                output.write(' '.join(tokenized_sentence))\n",
    "                output.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca3ef8ad-75b4-4ea2-bda6-529296178c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process file: wiki_08\n",
      "process file: wiki_18\n",
      "process file: wiki_12\n",
      "process file: wiki_17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "process file: wiki_03\n",
      "\n",
      "process file: wiki_14\n",
      "\n",
      "process file: wiki_16\n",
      "\n",
      "process file: wiki_04\n",
      "\n",
      "process file: wiki_09\n",
      "\n",
      "process file: wiki_10\n",
      "\n",
      "process file: wiki_13\n",
      "\n",
      "process file: wiki_02\n",
      "\n",
      "process file: wiki_11\n",
      "\n",
      "process file: wiki_07\n",
      "\n",
      "process file: wiki_19\n",
      "\n",
      "process file: wiki_05\n",
      "\n",
      "process file: wiki_15\n",
      "\n",
      "process file: wiki_06\n",
      "\n",
      "process file: wiki_00-checkpoint\n",
      "\n",
      "process file: wiki_00\n",
      "\n",
      "process file: wiki_01\n",
      "\n",
      "process file: wiki_03-checkpoint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pool = multiprocessing.Pool(processes=4)\n",
    "    \n",
    "    data = [] \n",
    "    # path: dirs 와 files 가 있는 경로\n",
    "    # dirs: path 아래의 폴더들\n",
    "    # files: path 아래의 파일들\n",
    "    for path, dirs, files in os.walk('enwiki_step1/'):\n",
    "        for file_name in files: \n",
    "            data.append( (path, file_name) )\n",
    "        \n",
    "    # 리스트의 각 요소에 대해 함수 적용\n",
    "    pool.map(corpus_preprocessor, data) \n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ab751-bc79-45b1-9c3f-868d1dd6f696",
   "metadata": {},
   "source": [
    "### W2V 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cdfcc68-c4be-4c65-9e3c-89230f03977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ukyoung/anaconda3/envs/ukyoung/lib/python3.6/site-packages/azure/storage/blob/_encryption.py:19: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography.hazmat.backends import default_backend\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "697a971f-03b5-4265-8481-5e470fffb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 공백 단위로 끊어서 토큰화\n",
    "class Loader(object):\n",
    "    def __init__(self, source_dir):\n",
    "        self.source_dir = source_dir\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for path, dirs, files in os.walk(self.source_dir):\n",
    "            for file in files:\n",
    "                with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        yield line.replace('\\\\n', '').replace('?', '').replace('!', '').replace(',', '').replace('.', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49542b2a-b94c-4086-9ab7-06fbb7cb98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_vocab = Loader('wiki_step2/') # 단어사전 생성을 위한 데이터\n",
    "sentences_train = Loader('wiki_step2/') # 학습을 위한 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f6993b7-32a7-4fba-8e41-4ace20686fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vector_size': 300, # 300차원 Embedding Vector 생성\n",
    "    'min_count': 5, # 단어 최소 빈도 수 제한 (빈도가 적은 단어들은 학습하지 않는다.)\n",
    "    \n",
    "    'batch_words': 10000, # 사전을 구축할때 한번에 읽을 단어 수\n",
    "    'workers': multiprocessing.cpu_count(), # 학습을 위한 프로세스의 수\n",
    "    \n",
    "    'sg': 1, # 0이면 CBOW, 1이면 skip-gram을 사용한다 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e55c3a95-f390-49ea-b14e-9c79f60222e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.corpus_count: 11692878\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(**config) # Word2vec 모델 생성\n",
    "model.build_vocab(sentences_vocab)\n",
    "print('model.corpus_count: {}'.format(model.corpus_count)) # corpus 개수 카운트\n",
    "\n",
    "model.train(sentences_train, total_examples=model.corpus_count, epochs=10) # Word2Vec training \n",
    "\n",
    "model.save('enwiki_w2v') # 모델을 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5156f534-8840-4018-b31b-d6efc0bef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로딩 \n",
    "load_model = Word2Vec.load('enwiki_w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9ec8451-b57e-47cb-89fb-336225da095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273940\n"
     ]
    }
   ],
   "source": [
    "# word vector 출력\n",
    "print(len(load_model.wv.vectors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
